---
title: "why spark"
date: 2025-01-01
---

# intro to apache spark: keeping it real with actual numbers

we've all heard the marketing: "big data processing", "distributed computing", "10x faster than mapreduce".

instead of abstract concepts, we'll look at real problems big companies solved with spark at scale, backed by actual metrics and code. 

hopefully by the end of this, you are shaking your head and be like "do i really need this complexity in my life?"

## why would you care

let's look at what tech companies are actually processing:

### uber
uber's scale [1]
- hundreds of millions of records flowing into hadoop daily
- handling 100+ data channels from ad networks
- managing 10k+ campaigns across multiple currencies
- processing complex data from social, display, and search ads

before spark, they were basically suffering [1]
- manual data processing = slow af and error-prone
- needed granular ROI metrics at every level
- schema mapping was a nightmare

their solution? they built a whole ecosystem around spark [1] [2]
- hadoop/spark-based euclid system for marketing analytics
- LSH on spark reduced fraud detection from 55 HOURS to 4 HOURS
- using literally everything spark offers: MLlib, SQL, streaming, RDD

### netflix
netflix's scale [3]
- processing ~500 billion events daily
- handling ~1.3 PB of data per day
- peak load: ~8 million events/second (~24 GB/sec)

their journey [3]
- ditched chukwa for kafka-based keystone pipeline [3]
- integrated spark for real-time processing [3]
- finally got their data routing game together [3]

### linkedin
linkedin's scale [4]
- ~20,000 hadoop nodes
- ONE EXABYTE of data
- yuge cluster: 10k nodes with 500 PB storage
- grew from 20 PB (2015) to 500 PB (2020)
- compute jumped from 2M to 50M gigabit-hours

### facebook
when fb tried to replace their hive pipeline with spark [5]
- processing 60TB+ of compressed input data
- handling 90TB+ shuffle and sort
- managing 250k tasks in a single job
- running on thousands of machines

the results were WILD:
- 4.5-6x CPU improvement
- 3-4x better resource utilization
- 5x faster end-to-end latency

but getting there wasn't easy. they had to fix:
- memory leaks in sorting
- shuffle service bottlenecks
- reliability issues with node reboots
- performance issues with large-scale operations

here's the type of spark code they had to deal with:
```python
# handling massive shuffles at fb scale
from pyspark.sql.functions import broadcast

# when you're shuffling 90TB, you need to tune everything
spark.conf.set("spark.shuffle.io.serverThreads", 32)
spark.conf.set("spark.shuffle.io.backLog", 8192)
spark.conf.set("spark.memory.fraction", 0.6)

# dealing with massive data skew
df = spark.read.parquet("massive_table")  # 60TB+
skewed_col = "entity_id"

# broadcast join with smaller table
small_table = spark.read.parquet("reference_data")
result = (
    df.join(
        broadcast(small_table),
        on=skewed_col
    )
    .repartition(
        2000,  # careful tuning needed
        skewed_col
    )
)
```

### facebook's NLP wizardry
fb also used spark for training massive language models [6]
- processing billions of n-grams
- handling 15x more data than their previous hive pipeline
- generating 19.2B n-grams in just hours
- used for video captioning and content quality

the real interesting part? how they handled data skew:

```python
# fb's progressive sharding technique for handling skewed n-grams
from pyspark.sql.functions import size, col

def process_ngrams_progressively(
    ngrams: DataFrame,
    shard_size_threshold: int = 1000000
) -> DataFrame:
    # first pass: single word history
    w1_shards = ngrams.repartition("first_word")
    
    # process small shards first
    small_shards = w1_shards.filter(
        size(col("ngrams")) < shard_size_threshold
    )
    
    # handle remaining large shards with 2-word history
    large_shards = w1_shards.filter(
        size(col("ngrams")) >= shard_size_threshold
    )
    w2_shards = large_shards.repartition("first_word", "second_word")
    
    # if needed, go to 3-word history for remaining huge shards
    # this handled their most skewed data
    
    return small_shards.union(w2_shards)
```

why this matters:
- old hive pipeline couldn't handle the scale
- spark made it 3-4x more resource efficient
- reduced end-to-end latency by 5x
- enabled training with 15x more data

## what makes spark different?

here's what these companies actually got from spark:

1. scalability that's not crap
- linkedin scaled from terabytes to petabytes [4]
- netflix handles billions of events without breaking a sweat [3]

2. real-time processing that works
- netflix achieving sub-minute latency [3]
- uber processing fraud detection 13x faster [2]

3. flexibility that matters
- uber using it for everything from ML to spatial data [2]
- netflix handling hundreds of event streams [3]

### old school vs new school

```python
# PAINFUL way: trying to process user sessions with pandas
import pandas as pd

# rip your ram
all_sessions = pd.read_parquet("user_sessions/*.parquet")  # 1TB+ of data

# good luck waiting for this
user_journey = (all_sessions
    .sort_values(['user_id', 'timestamp'])
    .groupby('user_id')
    .apply(lambda x: analyze_sequence(x))  # custom ml logic
)

# SPARK way: handling user journey analysis at scale
from pyspark.sql import Window
from pyspark.sql.functions import lag, collect_list

# distributed processing ftw
sessions = spark.read.parquet("user_sessions/*.parquet")

# window functions for sequential analysis
w = Window.partitionBy("user_id").orderBy("timestamp")

journey_features = sessions.withColumn(
    "prev_action", lag("action").over(w)
).withColumn(
    "time_since_last", 
    col("timestamp") - lag("timestamp").over(w)
)

# complex aggregations that can actually finish running
user_patterns = sessions.groupBy("user_id").agg(
    collect_list("action").alias("action_sequence"),
    # ml feature engineering
    array_sort(collect_set("device_type")).alias("devices_used"),
    count_distinct("session_id").alias("session_count")
)

# hadoop way (pov is at row level):
class WordCount(Mapper):
    def map(self, key, line, context):
        for word in line.split():
            context.write(word, 1)

class Reduce(Reducer):
    def reduce(self, word, counts, context):
        context.write(word, sum(counts))

# spark way (pov is at dataframe level):
text_file = spark.read.text("hdfs://...")
word_counts = (
    text_file
    .flatMap(lambda line: line.split())
    .map(lambda word: (word, 1))
    .reduceByKey(lambda a, b: a + b)
)
```

### big data processing

```python
# what people think big data is:
import pandas as pd
df = pd.read_csv("small_data.csv")

# what uber actually deals with:
events = spark.read.parquet("events")  # hundreds of millions of records [1]
campaigns = spark.read.parquet("campaigns")  # 10k+ campaigns [1]

# this innocent-looking join could kill your cluster
result = events.join(campaigns, "campaign_id")

# what you actually need:
result = (
    events
    .repartition(200, "campaign_id")
    .join(
        broadcast(campaigns),  # campaigns table is smaller
        "campaign_id"
    )
    .checkpoint()  # save intermediate results
)
```

### netflix-scale streaming

```python
# handling 8M events/sec [3] needs more nuanced logic:
streaming_df = (
    spark
    .readStream
    .format("kafka")
    .option("subscribe", "events")
    .load()
)

# you need windowing for real-time analytics
windowed_counts = (
    streaming_df
    .withWatermark("timestamp", "1 minute")
    .groupBy(
        window("timestamp", "1 minute"),
        "event_type"
    )
    .count()
)
```

## when do you actually need spark?

you probably need spark when:
1. your data doesn't fit on one machine (looking at linkedin's 500 PB) [4]
2. you have a massive amount of data to process in a batch for aggregation
3. you need real-time processing for big data (netflix's 8M events/sec) [3]
4. you're dealing with complex data pipelines (uber's marketing analytics) [1]

## the challenges

companies found these pain points:
1. infrastructure complexity
   - linkedin managing 20k nodes isn't a joke [4]
   - netflix dealing with hundreds of streams [3]

2. data complexity
   - uber handling multiple ad types and currencies [1]
   - schema mapping across different systems [1]

if you're thinking about using spark, start with:
1. understanding your actual scale needs
2. planning your data architecture
3. considering offline batch or real-time streaming requirements
4. evaluating infrastructure needs

## what's next?

but fr tho, this is just scratching the surface. in the next post we're gonna dive DEEP into:
- how spark actually executes your code (spoiler: it's wild)
- why shuffle operations make your cluster cry
- what REALLY happens when you hit that `.collect()` button
- the dark arts of memory management

these companies didn't start at exabyte scale. they grew into it. start small, but design for growth.

### rant

i can't believe i actually wrote all this

yall really think imma continue with this shxt lmao

i have totally cross-referenced all the metrics i cited btw totally not made up

### references

1. [Designing Euclid to Make Uber Engineering Marketing Savvy](https://www.uber.com/blog/euclid-marketing-engineering/)
2. [Detecting Abuse at Scale: Locality Sensitive Hashing at Uber Engineering](https://www.uber.com/en-GB/blog/lsh/)
3. [Evolution of the Netflix Data Pipeline](https://netflixtechblog.com/evolution-of-the-netflix-data-pipeline-da246ca36905)
4. [How LinkedIn Scales Its Analytical Data Platform](https://www.acceldata.io/blog/data-engineering-best-practices-linkedin)
5. [Apache Spark @Scale: A 60 TB+ production use case](https://engineering.fb.com/2016/08/31/core-infra/apache-spark-scale-a-60-tb-production-use-case/)
6. [Using Apache Spark for large-scale language model training](https://engineering.fb.com/2017/02/07/core-infra/using-apache-spark-for-large-scale-language-model-training/)
